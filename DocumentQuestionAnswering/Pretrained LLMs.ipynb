{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a068365b",
   "metadata": {},
   "source": [
    "### Model Name: Galactica-1.3B\n",
    "\n",
    "##### Pretrained Model that is Largely trained on Scientific Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e4b428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\AIXI\\AppData\\Local\\Temp\\ipykernel_7836\\1343144163.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\AIXI\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_7836\\\\1343144163.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'torch'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\AIXI\\AppData\\Local\\Temp\\ipykernel_7836\\1343144163.py\u001b[0m:\u001b[94m3\u001b[0m in \u001b[92m<module>\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\AIXI\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_7836\\\\1343144163.py'\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'torch'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import galai as gal\n",
    "\n",
    "model = gal.load_model(\"standard\", dtype=torch.float16)\n",
    "model.generate(\"Scaled dot product attention:\\n\\n\\\\[\")\n",
    "# Scaled dot product attention:\\n\\n\\\\[ \\\\displaystyle\\\\text{Attention}(Q,K,V)=\\\\text{softmax}(\\\\frac{QK^{T}}{\\\\sqrt{d_{k}}}%\\n)V \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c227b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d87d942837945789789428374224b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1186b1dc1e4cffb00e42776c1db2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e7fd04139648be8a9a0958c038c90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c076de6de704d0b860ced91c876349c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd82cecb8474f989479117d7a561e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIXI\\anaconda3\\envs\\newenv\\lib\\site-packages\\transformers\\generation\\utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture [START_REF] Attention is All you Need, Vaswani[END_REF] is a popular choice for sequence\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\")\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/galactica-1.3b\", device_map=\"auto\")\n",
    "\n",
    "input_text = \"The Transformer architecture [START_REF]\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2171cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cognitive architecture deals with the representation of the knowledge of the system. The knowledge is represented in the\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The cognitive architecture deals with\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc825931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba4d0392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do we need cognitive arc in AI?\n",
      "\n",
      "I am trying to understand the difference between cognitive\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Why do we need cognitive arc in AI?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "318063bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you understand cognitive architecture?\n",
      "\n",
      "I am a computer scientist and I am interested in cognitive architecture. I have read a lot of books and articles on cognitive architecture. I am not sure what is the best way to understand cognitive architecture.\n",
      "\n",
      "\n",
      "Answer:\n",
      "\n",
      "I think you are looking for [Cognitive Architecture\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Do you understand cognitive architecture?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_new_tokens= 60)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4a6b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")\n",
    "# model = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# input_text = \"The Transformer architecture [START_REF]\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "# print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b3c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\AIXI\\anaconda3\\envs\\newenv\\lib\\site-packages\\transformers\\models\\codegen\\modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorCompare.cpp:493.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world():\n",
      "    return \"Hello World!\"\n",
      "\n",
      "@app.route('/')\n",
      "def index():\n",
      "    return render_template('index.html')\n",
      "\n",
      "@app.route('/hello')\n",
      "def hello():\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-multi\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-multi\", device_map=\"auto\")\n",
    "\n",
    "text = \"def hello_world():\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(0)\n",
    "\n",
    "generated_ids = model.generate(input_ids, max_length=50)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583d16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-multi\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-multi\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd69d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\AIXI\\anaconda3\\envs\\newenv\\lib\\site-packages\\transformers\\models\\codegen\\modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorCompare.cpp:493.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me code to sort n numbers using merge sort algorithm.\n",
      "\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "#include <algorithm>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "void mergeSort(vector<int> &numbers, int left, int right)\n",
      "{\n",
      "    if (left < right)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = \"Give me code to sort n numbers using merge sort algorithm\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(0)\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "generated_ids = model.generate(input_ids, max_length=70,  attention_mask=attention_mask)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd83ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/Salesforce/codegen-2B-multi\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1604b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = query({\n",
    "\t\"inputs\": \"code to create neural network class using python: \",\n",
    "    \"parameters\": {'max_length': 200}\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c50d5f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'code to create neural network class using python: \\n\\nnetwork = MLP(img_w, img_h, img_d, n_hidden=50, layer_type=MLP.SOFTMAX)\\n\\n## train network using cross validation\\n# For neural networks, training accuracy, epsilon and learning rate are fixed as per default and can be changed by passing the values into \\n# MLP.setter and MLP.getter\\n\\nnetwork.train(10, 100, 1.0, 0.5, 10)\\n\\n## predict using a trained network\\nnetwork.predict(2)\\n\\n'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2514e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code to create neural network class using python: \n",
      "\n",
      "network = MLP(img_w, img_h, img_d, n_hidden=50, layer_type=MLP.SOFTMAX)\n",
      "\n",
      "## train network using cross validation\n",
      "# For neural networks, training accuracy, epsilon and learning rate are fixed as per default and can be changed by passing the values into \n",
      "# MLP.setter and MLP.getter\n",
      "\n",
      "network.train(10, 100, 1.0, 0.5, 10)\n",
      "\n",
      "## predict using a trained network\n",
      "network.predict(2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in output:\n",
    "    generated_text = item['generated_text']\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a653fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5845d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0f43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a13d253c",
   "metadata": {},
   "source": [
    "### Model name: CodeGen- Mono(350M)\n",
    "\n",
    "##### Natural language Text- To-Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88af1804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6ea308001e4c0da6fd3f2fa495e18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/240 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIXI\\anaconda3\\envs\\newenv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AIXI\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ef0ebeb5164fe8b90aefa86ef0a2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842b4b0ae6bb42bdb26e93c97260458c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0d5842707843feb365371d9c77cc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfad88842d048f7af282fc0a3805393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c2c9b1de02490dbaf2aee2de798a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a329548512de486ca2bb25007a96832c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6c9b4129a4437497387d75a60bc119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/797M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c8ac101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/Salesforce/codegen-350M-mono\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "371afcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = query({\n",
    "\t\"inputs\": \"code to create neural network class using python: \",\n",
    "    \"parameters\": {'max_length': 150}\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e22e5af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code to create neural network class using python: \n",
      "# https://github.com/adafraunn/neural_network_pytorch\n",
      "\"\"\"\n",
      "Modified versions of pre processers,\n",
      "e.g. preprocess_sentences()\n",
      "\"\"\"\n",
      "import os\n",
      "import re\n",
      "import pdb\n",
      "\n",
      "\n",
      "class TensorPreprocessor:\n",
      "    def __init__(self, path):\n",
      "        self.path = path\n",
      "\n",
      "    def preprocess_sentences(self, train, valid, test):\n",
      "\n",
      "        # TODO: Define preprocessed sentences preprocessor using following list\n",
      "        # TODO-1: Define preprocessor\n",
      "        \"\"\"\n",
      "        - Sentence format: Token,\n"
     ]
    }
   ],
   "source": [
    "for item in output:\n",
    "    generated_text = item['generated_text']\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70e18a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'code to create neural network class using python: \\nimport numpy as np\\nimport tensorflow as tf\\nimport keras\\n\\n\\nclass CNN:\\n\\n    def __init__(self):\\n        self.model = None\\n\\n    ### load a pickled model ###\\n\\n    # load a model\\n    def load_model(self, model_path):\\n        self.model = keras.models.load_model(model_path)\\n    # save a'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6704f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My gpu kernel",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
